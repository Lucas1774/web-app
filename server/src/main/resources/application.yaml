security:
  ip-address: ${SOME_IP_ADDRESS}

scheduler:
  market-data-cron: "0 0 0 * * *" # Every day at midnight UTC
  news-recommendations-cron: "0 15 9 * * *" # Every day before the market opens

spring:
  application:
    name: server
  profiles:
    active: ${SPRING_PROFILE}
  security:
    user:
      name: ${SPRING_USER}
      password: ${SPRING_PASSWORD}
    admin: ${ADMIN_USERS}
  datasource:
    username: ${DB_USERNAME}
    password: ${DB_PASSWORD}

ai:
  deployments:
    - name: gpt-4.1
      api-key: ${GITHUB_KEY}
      url: ${GITHUB_URL}
      model: openai/gpt-4.1
      temperature: 0
      max-tokens: 8000
      requests-per-minute: 5
      chunk-size: 5
      fix-me: false
    - name: gpt-4.1-2
      api-key: ${AI_FOUNDRY_KEY}
      url: ${GITHUB_URL}
      model: openai/gpt-4.1
      temperature: 0
      max-tokens: 8000
      requests-per-minute: 5
      chunk-size: 5
      fix-me: false
    - name: gpt-4.1-3
      api-key: ${AI_FOUNDRY_KEY_2}
      url: ${GITHUB_URL}
      model: openai/gpt-4.1
      temperature: 0
      max-tokens: 8000
      requests-per-minute: 5
      chunk-size: 5
      fix-me: false
    - name: gpt-4.1-4
      api-key: ${AI_FOUNDRY_KEY_3}
      url: ${GITHUB_URL}
      model: openai/gpt-4.1
      temperature: 0
      max-tokens: 8000
      requests-per-minute: 5
      chunk-size: 5
      fix-me: false
    - name: gpt-4.1-5
      api-key: ${AI_FOUNDRY_KEY_4}
      url: ${GITHUB_URL}
      model: openai/gpt-4.1
      temperature: 0
      max-tokens: 8000
      requests-per-minute: 5
      chunk-size: 5
      fix-me: false
    - name: gpt-4.1-6
      api-key: ${AI_FOUNDRY_KEY_5}
      url: ${GITHUB_URL}
      model: openai/gpt-4.1
      temperature: 0
      max-tokens: 8000
      requests-per-minute: 5
      chunk-size: 5
      fix-me: false
    - name: gpt-4o
      api-key: ${GITHUB_KEY}
      url: ${GITHUB_URL}
      model: openai/gpt-4o
      temperature: 0
      max-tokens: 8000
      requests-per-minute: 5
      chunk-size: 5
      fix-me: false
    - name: gpt-4.1-mini
      api-key: ${GITHUB_KEY}
      url: ${GITHUB_URL}
      model: openai/gpt-4.1-mini
      temperature: 0
      max-tokens: 8000
      requests-per-minute: 5
      chunk-size: 5
      fix-me: false
    - name: gpt-4o-mini
      api-key: ${GITHUB_KEY}
      url: ${GITHUB_URL}
      model: openai/gpt-4o-mini
      temperature: 0
      max-tokens: 8000
      requests-per-minute: 5
      chunk-size: 5
      fix-me: false
    - name: gpt-4.1-nano
      api-key: ${GITHUB_KEY}
      url: ${GITHUB_URL}
      model: openai/gpt-4.1-nano
      temperature: 0
      max-tokens: 8000
      requests-per-minute: 5
      chunk-size: 5
      fix-me: false
    - name: deepseek-v3
      api-key: ${GITHUB_KEY}
      url: ${GITHUB_URL}
      model: deepseek/deepseek-V3-0324
      temperature: 0
      max-tokens: 4000
      requests-per-minute: 1
      chunk-size: 2
      fix-me: false
    - name: grok-3
      api-key: ${GITHUB_KEY}
      url: ${GITHUB_URL}
      model: xai/grok-3
      temperature: 0
      max-tokens: 4000
      requests-per-minute: 1
      chunk-size: 2
      fix-me: false
    - name: grok-3-2
      api-key: ${AI_FOUNDRY_KEY}
      url: ${GITHUB_URL}
      model: xai/grok-3
      temperature: 0
      max-tokens: 4000
      requests-per-minute: 1
      chunk-size: 2
      fix-me: false
    - name: grok-3-3
      api-key: ${AI_FOUNDRY_KEY_2}
      url: ${GITHUB_URL}
      model: xai/grok-3
      temperature: 0
      max-tokens: 4000
      requests-per-minute: 1
      chunk-size: 2
      fix-me: false
    - name: grok-3-4
      api-key: ${AI_FOUNDRY_KEY_3}
      url: ${GITHUB_URL}
      model: xai/grok-3
      temperature: 0
      max-tokens: 4000
      requests-per-minute: 1
      chunk-size: 2
      fix-me: false
    - name: grok-3-5
      api-key: ${AI_FOUNDRY_KEY_4}
      url: ${GITHUB_URL}
      model: xai/grok-3
      temperature: 0
      max-tokens: 4000
      requests-per-minute: 1
      chunk-size: 2
      fix-me: false
    - name: grok-3-6
      api-key: ${AI_FOUNDRY_KEY_5}
      url: ${GITHUB_URL}
      model: xai/grok-3
      temperature: 0
      max-tokens: 4000
      requests-per-minute: 1
      chunk-size: 2
      fix-me: false
    - name: llama-3.3-70b
      api-key: ${GITHUB_KEY}
      url: ${GITHUB_URL}
      model: meta/llama-3.3-70b-instruct
      temperature: 0
      max-tokens: 8000
      requests-per-minute: 5
      chunk-size: 5
      fix-me: false
    - name: llama-3.1-405b
      api-key: ${GITHUB_KEY}
      url: ${GITHUB_URL}
      model: meta/meta-llama-3.1-405b-instruct
      temperature: 0
      max-tokens: 8000
      requests-per-minute: 5
      chunk-size: 5
      fix-me: false
    - name: llama-3.1-70b
      api-key: ${GITHUB_KEY}
      url: ${GITHUB_URL}
      model: meta/meta-llama-3.1-70b-instruct
      temperature: 0
      max-tokens: 8000
      requests-per-minute: 5
      chunk-size: 5
      fix-me: false
    - name: llama-3.1-8b
      api-key: ${GITHUB_KEY}
      url: ${GITHUB_URL}
      model: meta/meta-llama-3.1-8b-instruct
      temperature: 0
      max-tokens: 8000
      requests-per-minute: 5
      chunk-size: 5
      fix-me: false
    - name: mistral-large
      api-key: ${GITHUB_KEY}
      url: ${GITHUB_URL}
      model: mistral-ai/mistral-large-2411
      temperature: 0
      max-tokens: 8000
      requests-per-minute: 5
      chunk-size: 5
      fix-me: false
    - name: a21-jamba
      api-key: ${GITHUB_KEY}
      url: ${GITHUB_URL}
      model: ai21-labs/ai21-jamba-1.5-large
      temperature: 0
      max-tokens: 4096
      requests-per-minute: 5
      chunk-size: 2
      fix-me: false
    - name: command-r-plus-2024
      api-key: ${GITHUB_KEY}
      url: ${GITHUB_URL}
      model: cohere/cohere-command-r-plus-08-2024
      temperature: 0
      max-tokens: 8000
      requests-per-minute: 5
      chunk-size: 5
      fix-me: false
    - name: mistral-nemo
      api-key: ${GITHUB_KEY}
      url: ${GITHUB_URL}
      model: mistral-ai/mistral-nemo
      temperature: 0
      max-tokens: 8000
      requests-per-minute: 5
      chunk-size: 5
      fix-me: false
    - name: ministral-3b
      api-key: ${GITHUB_KEY}
      url: ${GITHUB_URL}
      model: mistral-ai/ministral-3b
      temperature: 0
      max-tokens: 8000
      requests-per-minute: 5
      chunk-size: 5
      fix-me: false
    - name: phi-4
      api-key: ${GITHUB_KEY}
      url: ${GITHUB_URL}
      model: microsoft/phi-4
      temperature: 0
      max-tokens: 8000
      requests-per-minute: 5
      chunk-size: 5
      fix-me: false
    - name: command-a
      api-key: ${GITHUB_KEY}
      url: ${GITHUB_URL}
      model: cohere/cohere-command-a
      temperature: 0
      max-tokens: 8000
      requests-per-minute: 5
      chunk-size: 5
      fix-me: false
    - name: deepseek-r1
      api-key: ${GITHUB_KEY}
      url: ${GITHUB_URL}
      model: deepseek/deepseek-r1
      temperature: 0
      max-tokens: 8000
      requests-per-minute: 1
      chunk-size: 5
      fix-me: false
    - name: deepseek-r1-0528
      api-key: ${GITHUB_KEY}
      url: ${GITHUB_URL}
      model: deepseek/deepseek-r1-0528
      temperature: 0
      max-tokens: 8000
      requests-per-minute: 1
      chunk-size: 5
      fix-me: false
    - name: command-r-plus
      api-key: ${GITHUB_KEY}
      url: ${GITHUB_URL}
      model: cohere/cohere-command-r-plus
      temperature: 0
      max-tokens: 8000
      requests-per-minute: 5
      chunk-size: 5
      fix-me: false
    - name: o4-mini
      api-key: ${GITHUB_KEY}
      url: ${GITHUB_URL}
      model: openai/o4-mini
      temperature: 0
      max-tokens: 8000
      requests-per-minute: 5
      chunk-size: 5
      fix-me: false
    - name: o3
      api-key: ${GITHUB_KEY}
      url: ${GITHUB_URL}
      model: openai/o3
      temperature: 0
      max-tokens: 8000
      requests-per-minute: 5
      chunk-size: 5
      fix-me: false
    - name: o3-mini
      api-key: ${GITHUB_KEY}
      url: ${GITHUB_URL}
      model: openai/o3-mini
      temperature: 0
      max-tokens: 8000
      requests-per-minute: 5
      chunk-size: 5
      fix-me: false
    - name: o1
      api-key: ${GITHUB_KEY}
      url: ${GITHUB_URL}
      model: openai/o1
      temperature: 0
      max-tokens: 8000
      requests-per-minute: 5
      chunk-size: 5
      fix-me: false
    - name: o1-preview
      api-key: ${GITHUB_KEY}
      url: ${GITHUB_URL}
      model: openai/o1-preview
      temperature: 0
      max-tokens: 8000
      requests-per-minute: 5
      chunk-size: 5
      fix-me: false
    - name: o1-mini
      api-key: ${GITHUB_KEY}
      url: ${GITHUB_URL}
      model: openai/o1-mini
      temperature: 0
      max-tokens: 8000
      requests-per-minute: 5
      chunk-size: 5
      fix-me: false

finnhub:
  api-keys: ${FINNHUB_KEYS}
  endpoint: ${FINNHUB_ENDPOINT}

twelve-data:
  api-key: ${TWELVE_DATA_KEY}
  endpoint: ${TWELVE_DATA_ENDPOINT}

yahoo:
  news:
    endpoint: ${YAHOO_NEWS_ENDPOINT}
  market:
    endpoint: ${YAHOO_MARKET_ENDPOINT}

sentiment:
  url: ${SENTIMENT_URL}
